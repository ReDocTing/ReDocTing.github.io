<!DOCTYPE html><html lang="CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>阅读列表 | ReDocTingのBlog</title><meta name="author" content="ReDocTing"><meta name="copyright" content="ReDocTing"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="个人计划阅读内容，读完会添加笔记连接，阅读列表持续更新。">
<meta property="og:type" content="article">
<meta property="og:title" content="阅读列表">
<meta property="og:url" content="https://redocting.com/2024/06/13/paper/index.html">
<meta property="og:site_name" content="ReDocTingのBlog">
<meta property="og:description" content="个人计划阅读内容，读完会添加笔记连接，阅读列表持续更新。">
<meta property="og:locale">
<meta property="og:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png">
<meta property="article:published_time" content="2024-06-13T10:10:22.715Z">
<meta property="article:modified_time" content="2024-06-13T10:45:39.643Z">
<meta property="article:author" content="ReDocTing">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://redocting.com/2024/06/13/paper/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '阅读列表',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-06-13 19:45:39'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">3</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr class="custom-hr"/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="ReDocTingのBlog"><span class="site-name">ReDocTingのBlog</span></a></span><div id="menus"><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">阅读列表</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-06-13T10:10:22.715Z" title="Created 2024-06-13 19:10:22">2024-06-13</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-06-13T10:45:39.643Z" title="Updated 2024-06-13 19:45:39">2024-06-13</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="阅读列表"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>个人计划阅读内容，读完会添加笔记连接，阅读列表持续更新。</p>
<h1 id="阅读列表"><a href="#阅读列表" class="headerlink" title="阅读列表"></a>阅读列表</h1><h3 id="计算机视觉-CNN"><a href="#计算机视觉-CNN" class="headerlink" title="计算机视觉 - CNN"></a>计算机视觉 - CNN</h3><table>
<thead>
<tr>
<th>已读</th>
<th>年份</th>
<th>名字</th>
<th>简介</th>
<th>引用</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td>2012</td>
<td><a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">AlexNet</a></td>
<td>深度学习热潮的奠基作</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/abd1c342495432171beb7ca8fd9551ef13cbd0ff?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2014</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1409.1556.pdf">VGG</a></td>
<td>使用 3x3 卷积构造更深的网络</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Very-Deep-Convolutional-Networks-for-Large-Scale-Simonyan-Zisserman/eb42cf88027de515750f230b23b1a057dc782108"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/eb42cf88027de515750f230b23b1a057dc782108?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2014</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1409.4842.pdf">GoogleNet</a></td>
<td>使用并行架构构造更深的网络</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Going-deeper-with-convolutions-Szegedy-Liu/e15cf50aa89fee8535703b9f9512fca5bfc43327"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/e15cf50aa89fee8535703b9f9512fca5bfc43327?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2015</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1512.03385.pdf">ResNet</a></td>
<td>构建深层网络都要有的残差连接。</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Deep-Residual-Learning-for-Image-Recognition-He-Zhang/2c03df8b48bf3fa39054345bafabfeff15bfd11d"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2017</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1704.04861.pdf">MobileNet</a></td>
<td>适合终端设备的小CNN</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/MobileNets%3A-Efficient-Convolutional-Neural-Networks-Howard-Zhu/3647d6d0f151dc05626449ee09cc7bce55be497e"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/3647d6d0f151dc05626449ee09cc7bce55be497e?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2019</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1905.11946.pdf">EfficientNet</a></td>
<td>通过架构搜索得到的CNN</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/EfficientNet%3A-Rethinking-Model-Scaling-for-Neural-Tan-Le/4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2021</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2110.07641.pdf">Non-deep networks</a></td>
<td>让不深的网络也能在ImageNet刷到SOTA</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Non-deep-Networks-Goyal-Bochkovskiy/0d7f6086772079bc3e243b7b375a9ca1a517ba8b"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/0d7f6086772079bc3e243b7b375a9ca1a517ba8b?fields=citationCount" alt="citation"></a></td>
</tr>
</tbody></table>
<h3 id="计算机视觉-Transformer"><a href="#计算机视觉-Transformer" class="headerlink" title="计算机视觉 - Transformer"></a>计算机视觉 - Transformer</h3><table>
<thead>
<tr>
<th>已读</th>
<th>年份</th>
<th>名字</th>
<th>简介</th>
<th>引用</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td>2020</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.11929.pdf">ViT</a></td>
<td>Transformer杀入CV界</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/An-Image-is-Worth-16x16-Words%3A-Transformers-for-at-Dosovitskiy-Beyer/7b15fa1b8d413fbe14ef7a97f651f47f5aff3903"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/7b15fa1b8d413fbe14ef7a97f651f47f5aff3903?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2021</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.14030.pdf">Swin Transformer</a></td>
<td>多层次的Vision Transformer</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Swin-Transformer%3A-Hierarchical-Vision-Transformer-Liu-Lin/c8b25fab5608c3e033d34b4483ec47e68ba109b7"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/c8b25fab5608c3e033d34b4483ec47e68ba109b7?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2021</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2105.01601.pdf">MLP-Mixer</a></td>
<td>使用MLP替换self-attention</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/MLP-Mixer%3A-An-all-MLP-Architecture-for-Vision-Tolstikhin-Houlsby/2def61f556f9a5576ace08911496b7c7e4f970a4"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/2def61f556f9a5576ace08911496b7c7e4f970a4?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2021</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2111.06377.pdf">MAE</a></td>
<td>BERT的CV版</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Masked-Autoencoders-Are-Scalable-Vision-Learners-He-Chen/c1962a8cf364595ed2838a097e9aa7cd159d3118"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/c1962a8cf364595ed2838a097e9aa7cd159d3118?fields=citationCount" alt="citation"></a></td>
</tr>
</tbody></table>
<h3 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h3><table>
<thead>
<tr>
<th>已读</th>
<th>年份</th>
<th>名字</th>
<th>简介</th>
<th>引用</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td>2014</td>
<td><a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf">GAN</a></td>
<td>生成模型的开创工作</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Generative-Adversarial-Nets-Goodfellow-Pouget-Abadie/54e325aee6b2d476bbbb88615ac15e251c6e8214"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/54e325aee6b2d476bbbb88615ac15e251c6e8214?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2015</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1511.06434.pdf">DCGAN</a></td>
<td>使用CNN的GAN</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Unsupervised-Representation-Learning-with-Deep-Radford-Metz/8388f1be26329fa45e5807e968a641ce170ea078"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/8388f1be26329fa45e5807e968a641ce170ea078?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2016</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1611.07004.pdf">pix2pix</a></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Image-to-Image-Translation-with-Conditional-Isola-Zhu/8acbe90d5b852dadea7810345451a99608ee54c7"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/8acbe90d5b852dadea7810345451a99608ee54c7?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2016</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1609.04802.pdf">SRGAN</a></td>
<td>图片超分辨率</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Photo-Realistic-Single-Image-Super-Resolution-Using-Ledig-Theis/df0c54fe61f0ffb9f0e36a17c2038d9a1964cba3"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/df0c54fe61f0ffb9f0e36a17c2038d9a1964cba3?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2017</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1701.07875">WGAN</a></td>
<td>训练更加容易</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Wasserstein-GAN-Arjovsky-Chintala/2f85b7376769473d2bed56f855f115e23d727094"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/2f85b7376769473d2bed56f855f115e23d727094?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2017</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.10593">CycleGAN</a></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Unpaired-Image-to-Image-Translation-Using-Networks-Zhu-Park/c43d954cf8133e6254499f3d68e45218067e4941"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/c43d954cf8133e6254499f3d68e45218067e4941?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2018</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.04948">StyleGAN</a></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/A-Style-Based-Generator-Architecture-for-Generative-Karras-Laine/ceb2ebef0b41e31c1a21b28c2734123900c005e2"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/ceb2ebef0b41e31c1a21b28c2734123900c005e2?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2019</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.04958.pdf">StyleGAN2</a></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Analyzing-and-Improving-the-Image-Quality-of-Karras-Laine/f3e3d1f86a534a3654d0ee263142e44f4e2c61e9"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/f3e3d1f86a534a3654d0ee263142e44f4e2c61e9?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2020</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2006.11239.pdf">DDPM</a></td>
<td>Diffusion Models</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Denoising-Diffusion-Probabilistic-Models-Ho-Jain/289db3be7bf77e06e75541ba93269de3d604ac72"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/289db3be7bf77e06e75541ba93269de3d604ac72?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2021</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2102.09672.pdf">Improved DDPM</a></td>
<td>改进的 DDPM</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Improved-Denoising-Diffusion-Probabilistic-Models-Nichol-Dhariwal/de18baa4964804cf471d85a5a090498242d2e79f"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/de18baa4964804cf471d85a5a090498242d2e79f?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2021</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2105.05233.pdf">Guided Diffusion Models</a></td>
<td>号称超越 GAN</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Diffusion-Models-Beat-GANs-on-Image-Synthesis-Dhariwal-Nichol/64ea8f180d0682e6c18d1eb688afdb2027c02794"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/64ea8f180d0682e6c18d1eb688afdb2027c02794?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2021</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.12423.pdf">StyleGAN3</a></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Alias-Free-Generative-Adversarial-Networks-Karras-Aittala/c1ff08b59f00c44f34dfdde55cd53370733a2c19"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/c1ff08b59f00c44f34dfdde55cd53370733a2c19?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2022</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.06125.pdf">DALL.E 2</a></td>
<td>CLIP + Diffusion models，文本生成图像新高度</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Hierarchical-Text-Conditional-Image-Generation-with-Ramesh-Dhariwal/c57293882b2561e1ba03017902df9fc2f289dea2"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/c57293882b2561e1ba03017902df9fc2f289dea2?fields=citationCount" alt="citation"></a></td>
</tr>
</tbody></table>
<h3 id="计算机视觉-Object-Detection"><a href="#计算机视觉-Object-Detection" class="headerlink" title="计算机视觉 - Object Detection"></a>计算机视觉 - Object Detection</h3><table>
<thead>
<tr>
<th>已读</th>
<th>年份</th>
<th>名字</th>
<th>简介</th>
<th>引用</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td>2014</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1311.2524v5.pdf">R-CNN</a></td>
<td>Two-stage</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/2f4df08d9072fc2ac181b7fced6a245315ce05c8"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/2f4df08d9072fc2ac181b7fced6a245315ce05c8?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2015</td>
<td><a target="_blank" rel="noopener" href="http://arxiv.org/abs/1504.08083v2">Fast R-CNN</a></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/7ffdbc358b63378f07311e883dddacc9faeeaf4b"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/7ffdbc358b63378f07311e883dddacc9faeeaf4b?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2015</td>
<td><a target="_blank" rel="noopener" href="http://arxiv.org/abs/1506.01497v3">Faster R-CNN</a></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/424561d8585ff8ebce7d5d07de8dbf7aae5e7270"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/424561d8585ff8ebce7d5d07de8dbf7aae5e7270?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2016</td>
<td><a target="_blank" rel="noopener" href="http://arxiv.org/abs/1512.02325v5">SSD</a></td>
<td>Single stage</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2016</td>
<td><a target="_blank" rel="noopener" href="http://arxiv.org/abs/1506.02640v5">YOLO</a></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/f8e79ac0ea341056ef20f2616628b3e964764cfd"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/f8e79ac0ea341056ef20f2616628b3e964764cfd?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2017</td>
<td><a target="_blank" rel="noopener" href="http://arxiv.org/abs/1703.06870v3">Mask R-CNN</a></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/ea99a5535388196d0d44be5b4d7dd02029a43bb2"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/ea99a5535388196d0d44be5b4d7dd02029a43bb2?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2017</td>
<td><a target="_blank" rel="noopener" href="http://arxiv.org/abs/1612.08242v1">YOLOv2</a></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/7d39d69b23424446f0400ef603b2e3e22d0309d6"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/7d39d69b23424446f0400ef603b2e3e22d0309d6?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2018</td>
<td><a target="_blank" rel="noopener" href="http://arxiv.org/abs/1804.02767v1">YOLOv3</a></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/e4845fb1e624965d4f036d7fd32e8dcdd2408148"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/e4845fb1e624965d4f036d7fd32e8dcdd2408148?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2019</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.07850.pdf">CenterNet</a></td>
<td>Anchor free</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Objects-as-Points-Zhou-Wang/6a2e2fd1b5bb11224daef98b3fb6d029f68a73f2"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/6a2e2fd1b5bb11224daef98b3fb6d029f68a73f2?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2020</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2005.12872.pdf">DETR</a></td>
<td>Transformer</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/End-to-End-Object-Detection-with-Transformers-Carion-Massa/962dc29fdc3fbdc5930a10aba114050b82fe5a3e"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/962dc29fdc3fbdc5930a10aba114050b82fe5a3e?fields=citationCount" alt="citation"></a></td>
</tr>
</tbody></table>
<h3 id="计算机视觉-对比学习"><a href="#计算机视觉-对比学习" class="headerlink" title="计算机视觉 - 对比学习"></a>计算机视觉 - 对比学习</h3><table>
<thead>
<tr>
<th>已读</th>
<th>年份</th>
<th>名字</th>
<th>简介</th>
<th>引用</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td>2018</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1805.01978.pdf">InstDisc</a></td>
<td>提出实例判别和memory bank做对比学习</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Unsupervised-Feature-Learning-via-Non-parametric-Wu-Xiong/155b7782dbd713982a4133df3aee7adfd0b6b304"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/155b7782dbd713982a4133df3aee7adfd0b6b304?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2018</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1807.03748.pdf">CPC</a></td>
<td>对比预测编码，图像语音文本强化学习全都能做</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Representation-Learning-with-Contrastive-Predictive-Oord-Li/b227f3e4c0dc96e5ac5426b85485a70f2175a205"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/b227f3e4c0dc96e5ac5426b85485a70f2175a205?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2019</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.03436.pdf">InvaSpread</a></td>
<td>一个编码器的端到端对比学习</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Unsupervised-Embedding-Learning-via-Invariant-and-Ye-Zhang/e4bde6fe33b6c2cf9d1647ac0b041f7d1ba29c5b"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/e4bde6fe33b6c2cf9d1647ac0b041f7d1ba29c5b?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2019</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1906.05849.pdf">CMC</a></td>
<td>多视角下的对比学习</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Contrastive-Multiview-Coding-Tian-Krishnan/97f4d09175705be4677d675fa27e55defac44800"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/97f4d09175705be4677d675fa27e55defac44800?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2019</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1911.05722.pdf">MoCov1</a></td>
<td>无监督训练效果也很好</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Momentum-Contrast-for-Unsupervised-Visual-Learning-He-Fan/ec46830a4b275fd01d4de82bffcabe6da086128f"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/ec46830a4b275fd01d4de82bffcabe6da086128f?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2020</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2002.05709.pdf">SimCLRv1</a></td>
<td>简单的对比学习 (数据增强 + MLP head + 大batch训练久)</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/A-Simple-Framework-for-Contrastive-Learning-of-Chen-Kornblith/34733eaf66007516347a40ad5d9bbe1cc9dacb6b"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/34733eaf66007516347a40ad5d9bbe1cc9dacb6b?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2020</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.04297.pdf">MoCov2</a></td>
<td>MoCov1 + improvements from SimCLRv1</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Improved-Baselines-with-Momentum-Contrastive-Chen-Fan/a1b8a8df281bbaec148a897927a49ea47ea31515"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/a1b8a8df281bbaec148a897927a49ea47ea31515?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2020</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2006.10029.pdf">SimCLRv2</a></td>
<td>大的自监督预训练模型很适合做半监督学习</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Big-Self-Supervised-Models-are-Strong-Learners-Chen-Kornblith/3e7f5f4382ac6f9c4fef6197dd21abf74456acd1"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/3e7f5f4382ac6f9c4fef6197dd21abf74456acd1?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2020</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2006.07733.pdf">BYOL</a></td>
<td>不需要负样本的对比学习</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Bootstrap-Your-Own-Latent%3A-A-New-Approach-to-Grill-Strub/38f93092ece8eee9771e61c1edaf11b1293cae1b"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/38f93092ece8eee9771e61c1edaf11b1293cae1b?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2020</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2006.09882.pdf">SWaV</a></td>
<td>聚类对比学习</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Unsupervised-Learning-of-Visual-Features-by-Cluster-Caron-Misra/10161d83d29fc968c4612c9e9e2b61a2fc25842e"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/10161d83d29fc968c4612c9e9e2b61a2fc25842e?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2020</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2011.10566.pdf">SimSiam</a></td>
<td>化繁为简的孪生表征学习</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Exploring-Simple-Siamese-Representation-Learning-Chen-He/0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2021</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.02057.pdf">MoCov3</a></td>
<td>如何更稳定的自监督训练ViT</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/An-Empirical-Study-of-Training-Self-Supervised-Chen-Xie/739ceacfafb1c4eaa17509351b647c773270b3ae"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/739ceacfafb1c4eaa17509351b647c773270b3ae?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2021</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.14294.pdf">DINO</a></td>
<td>transformer加自监督在视觉也很香</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Emerging-Properties-in-Self-Supervised-Vision-Caron-Touvron/ad4a0938c48e61b7827869e4ac3baffd0aefab35"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/ad4a0938c48e61b7827869e4ac3baffd0aefab35?fields=citationCount" alt="citation"></a></td>
</tr>
</tbody></table>
<h3 id="计算机视觉-视频理解"><a href="#计算机视觉-视频理解" class="headerlink" title="计算机视觉 - 视频理解"></a>计算机视觉 - 视频理解</h3><table>
<thead>
<tr>
<th>已读</th>
<th>年份</th>
<th>名字</th>
<th>简介</th>
<th>引用</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td>2014</td>
<td><a target="_blank" rel="noopener" href="https://cs.stanford.edu/people/karpathy/deepvideo/">DeepVideo</a></td>
<td>提出sports1M数据集，用深度学习做视频理解</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Large-Scale-Video-Classification-with-Convolutional-Karpathy-Toderici/6d4c9c923e9f145d1c01a2de2afc38ec23c44253"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/6d4c9c923e9f145d1c01a2de2afc38ec23c44253?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2014</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1406.2199.pdf">Two-stream</a></td>
<td>引入光流做时序建模，神经网络首次超越手工特征</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Two-Stream-Convolutional-Networks-for-Action-in-Simonyan-Zisserman/67dccc9a856b60bdc4d058d83657a089b8ad4486"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/67dccc9a856b60bdc4d058d83657a089b8ad4486?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2014</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1412.0767.pdf">C3D</a></td>
<td>比较深的3D-CNN做视频理解</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Learning-Spatiotemporal-Features-with-3D-Networks-Tran-Bourdev/d25c65d261ea0e6a458be4c50c40ffe5bc508f77"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/d25c65d261ea0e6a458be4c50c40ffe5bc508f77?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2015</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1503.08909.pdf">Beyond-short-snippets</a></td>
<td>尝试使用LSTM</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Beyond-short-snippets%3A-Deep-networks-for-video-Ng-Hausknecht/5418b2a482720e013d487a385c26fae0f017c6a6"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/5418b2a482720e013d487a385c26fae0f017c6a6?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2016</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1604.06573.pdf">Convolutional fusion</a></td>
<td>做early fusion来加强时空间建模</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Convolutional-Two-Stream-Network-Fusion-for-Video-Feichtenhofer-Pinz/9d9aced120e530484609164c836da64548693484"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/9d9aced120e530484609164c836da64548693484?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2016</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1608.00859.pdf">TSN</a></td>
<td>超级有效的视频分段建模，bag of tricks in video</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Temporal-Segment-Networks%3A-Towards-Good-Practices-Wang-Xiong/ea3d7de6c0880e14455b9acb28f1bc1234321456"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/ea3d7de6c0880e14455b9acb28f1bc1234321456?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2017</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1705.07750.pdf">I3D</a></td>
<td>提出Kinetics数据集，膨胀2D网络到3D，开启3D-CNN时代</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Quo-Vadis%2C-Action-Recognition-A-New-Model-and-the-Carreira-Zisserman/b61a3f8b80bbd44f24544dc915f52fd30bbdf485"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/b61a3f8b80bbd44f24544dc915f52fd30bbdf485?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2017</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1711.11248.pdf">R2+1D</a></td>
<td>拆分3D卷积核，使3D网络容易优化</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/A-Closer-Look-at-Spatiotemporal-Convolutions-for-Tran-Wang/89c3050522a0bb9820c32dc7444e003ef0d3e2e4"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/89c3050522a0bb9820c32dc7444e003ef0d3e2e4?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2017</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1711.07971.pdf">Non-local</a></td>
<td>引入自注意力做视觉问题</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Non-local-Neural-Networks-Wang-Girshick/8899094797e82c5c185a0893896320ef77f60e64"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/8899094797e82c5c185a0893896320ef77f60e64?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2018</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1812.03982.pdf">SlowFast</a></td>
<td>快慢两支提升效率</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/SlowFast-Networks-for-Video-Recognition-Feichtenhofer-Fan/8b47b9c3c35b2b2a78bff7822605b3040f87d699"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/8b47b9c3c35b2b2a78bff7822605b3040f87d699?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2021</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2102.05095.pdf">TimeSformer</a></td>
<td>视频中第一个引入transformer，开启video transformer时代</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Is-Space-Time-Attention-All-You-Need-for-Video-Bertasius-Wang/c143ea9e30b1f2d93a9c060253845423f9e60e1f"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/c143ea9e30b1f2d93a9c060253845423f9e60e1f?fields=citationCount" alt="citation"></a></td>
</tr>
</tbody></table>
<h3 id="多模态学习"><a href="#多模态学习" class="headerlink" title="多模态学习"></a>多模态学习</h3><table>
<thead>
<tr>
<th>已读</th>
<th>年份</th>
<th>名字</th>
<th>简介</th>
<th>引用</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td>2021</td>
<td><a target="_blank" rel="noopener" href="https://openai.com/blog/clip/">CLIP</a></td>
<td>图片和文本之间的对比学习</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Learning-Transferable-Visual-Models-From-Natural-Radford-Kim/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2021</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2102.03334.pdf">ViLT</a></td>
<td>第一个摆脱了目标检测的视觉文本模型</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/ViLT%3A-Vision-and-Language-Transformer-Without-or-Kim-Son/0839722fb5369c0abaff8515bfc08299efc790a1"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/0839722fb5369c0abaff8515bfc08299efc790a1?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2021</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.13921.pdf">ViLD</a></td>
<td>CLIP蒸馏帮助开集目标检测</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Open-vocabulary-Object-Detection-via-Vision-and-Gu-Lin/cf9b8da26d9b92e75ba49616ed2a1033f59fce14"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/cf9b8da26d9b92e75ba49616ed2a1033f59fce14?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2021</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.03857.pdf">GLIP</a></td>
<td>联合目标检测和文本定位</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Grounded-Language-Image-Pre-training-Li-Zhang/5341b412383c43f4a693ad63ec4489e3ec7688c8"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/5341b412383c43f4a693ad63ec4489e3ec7688c8?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2021</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.08860.pdf">CLIP4Clip</a></td>
<td>拿CLIP直接做视频文本retrieval</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/CLIP4Clip%3A-An-Empirical-Study-of-CLIP-for-End-to-Luo-Ji/281ad83e06d731d5d686acf07cd701576f1188c4"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/281ad83e06d731d5d686acf07cd701576f1188c4?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2021</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2109.08472.pdf">ActionCLIP</a></td>
<td>用多模态对比学习有监督的做视频动作分类</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/ActionCLIP%3A-A-New-Paradigm-for-Video-Action-Wang-Xing/dc05240a06326b5b1664f7e8c95c330b08cd0349"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/dc05240a06326b5b1664f7e8c95c330b08cd0349?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2021</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.02413.pdf">PointCLIP</a></td>
<td>3D变2D，巧妙利用CLIP做点云</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/PointCLIP%3A-Point-Cloud-Understanding-by-CLIP-Zhang-Guo/f3ce9ba3fcec362b70263a7ed63d9404975496a0"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/f3ce9ba3fcec362b70263a7ed63d9404975496a0?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2022</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2201.03546.pdf">LSeg</a></td>
<td>有监督的开集分割</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Language-driven-Semantic-Segmentation-Li-Weinberger/cc9826c222ac1e81b4b374dd9e0df130f298b1e8"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/cc9826c222ac1e81b4b374dd9e0df130f298b1e8?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2022</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2202.11094.pdf">GroupViT</a></td>
<td>只用图像文本对也能无监督做分割</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/GroupViT%3A-Semantic-Segmentation-Emerges-from-Text-Xu-Mello/0b5f27a5766c5d1394a6282ad94fec21d620bd6b"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/0b5f27a5766c5d1394a6282ad94fec21d620bd6b?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2022</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2202.05822.pdf">CLIPasso</a></td>
<td>CLIP跨界生成简笔画</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/CLIPasso%3A-Semantically-Aware-Object-Sketching-Vinker-Pajouheshgar/9dec819778bebae4a468c7813f7638534c826f52"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/9dec819778bebae4a468c7813f7638534c826f52?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2022</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.01077.pdf">DepthCLIP</a></td>
<td>用文本跨界估计深度</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Can-Language-Understand-Depth-Zhang-Zeng/9d0afe58801fe9e5537902e853d6e9e385340a92"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/9d0afe58801fe9e5537902e853d6e9e385340a92?fields=citationCount" alt="citation"></a></td>
</tr>
</tbody></table>
<h3 id="自然语言处理-Transformer"><a href="#自然语言处理-Transformer" class="headerlink" title="自然语言处理 - Transformer"></a>自然语言处理 - Transformer</h3><table>
<thead>
<tr>
<th>已读</th>
<th>年份</th>
<th>名字</th>
<th>简介</th>
<th>引用</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td>2017</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Transformer</a></td>
<td>继MLP、CNN、RNN后的第四大类架构</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2018</td>
<td><a target="_blank" rel="noopener" href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">GPT</a></td>
<td>使用 Transformer 解码器来做预训练</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2018</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">BERT</a></td>
<td>Transformer一统NLP的开始</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/df2b0e26d0599ce3e70df8a9da02e51594e0e992?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2019</td>
<td><a target="_blank" rel="noopener" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a></td>
<td>更大的 GPT 模型，朝着zero-shot learning迈了一大步</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/9405cc0d6169988371b2755e573cc28650d14dfe?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2020</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.14165">GPT-3</a></td>
<td>100倍更大的 GPT-2，few-shot learning效果显著</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Language-Models-are-Few-Shot-Learners-Brown-Mann/6b85b63579a916f705a8e10a49bd8d849d91b1fc"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/6b85b63579a916f705a8e10a49bd8d849d91b1fc?fields=citationCount" alt="citation"></a></td>
</tr>
</tbody></table>
<h3 id="系统"><a href="#系统" class="headerlink" title="系统"></a>系统</h3><table>
<thead>
<tr>
<th>已读</th>
<th>年份</th>
<th>名字</th>
<th>简介</th>
<th>引用</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td>2014</td>
<td><a target="_blank" rel="noopener" href="https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf">参数服务器</a></td>
<td>支持千亿参数的传统机器学习模型</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Scaling-Distributed-Machine-Learning-with-the-Li-Andersen/0de0c3240bda7972bd0a3c8369ebc4b4f2e4f9c2"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/0de0c3240bda7972bd0a3c8369ebc4b4f2e4f9c2?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2018</td>
<td><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2019/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf">GPipe</a></td>
<td>流水线（Pipeline）并行</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/GPipe%3A-Efficient-Training-of-Giant-Neural-Networks-Huang-Cheng/c18663fea10c8a303d045fd2c1f33cacf9b73ca3"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/c18663fea10c8a303d045fd2c1f33cacf9b73ca3?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2019</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053.pdf">Megatron-LM</a></td>
<td>张量（Tensor）并行</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Megatron-LM%3A-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary/8323c591e119eb09b28b29fd6c7bc76bd889df7a"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/8323c591e119eb09b28b29fd6c7bc76bd889df7a?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2019</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1910.02054.pdf">Zero</a></td>
<td>参数分片</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/ZeRO%3A-Memory-optimizations-Toward-Training-Trillion-Rajbhandari-Rasley/00c957711b12468cb38424caccdf5291bb354033"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/00c957711b12468cb38424caccdf5291bb354033?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2022</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.12533.pdf">Pathways</a></td>
<td>将Jax拓展到上千TPU核上</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Pathways%3A-Asynchronous-Distributed-Dataflow-for-ML-Barham-Chowdhery/512e9aa873a1cd7eb61ce1f25ca7df6acb7e2352"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/512e9aa873a1cd7eb61ce1f25ca7df6acb7e2352?fields=citationCount" alt="citation"></a></td>
</tr>
</tbody></table>
<h3 id="图神经网络"><a href="#图神经网络" class="headerlink" title="图神经网络"></a>图神经网络</h3><table>
<thead>
<tr>
<th>已读</th>
<th>年份</th>
<th>名字</th>
<th>简介</th>
<th>引用</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td>2021</td>
<td><a target="_blank" rel="noopener" href="https://distill.pub/2021/gnn-intro/">图神经网络介绍</a></td>
<td>GNN的可视化介绍</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/A-Gentle-Introduction-to-Graph-Neural-Networks-S%C3%A1nchez-Lengeling-Reif/2c0e0440882a42be752268d0b64243243d752a74"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/2c0e0440882a42be752268d0b64243243d752a74?fields=citationCount" alt="citation"></a></td>
</tr>
</tbody></table>
<h3 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h3><table>
<thead>
<tr>
<th>已读</th>
<th>年份</th>
<th>名字</th>
<th>简介</th>
<th>引用</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td>2014</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1412.6980">Adam</a></td>
<td>深度学习里最常用的优化算法之一</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Adam%3A-A-Method-for-Stochastic-Optimization-Kingma-Ba/a6cb366736791bcccc5c8639de5a8f9636bf87e8"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2016</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.03530">为什么超大的模型泛化性不错</a></td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Understanding-deep-learning-requires-rethinking-Zhang-Bengio/54ddb00fa691728944fd8becea90a373d21597cf"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/54ddb00fa691728944fd8becea90a373d21597cf?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2017</td>
<td><a target="_blank" rel="noopener" href="https://distill.pub/2017/momentum/">为什么Momentum有效</a></td>
<td>Distill的可视化介绍</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Why-Momentum-Really-Works-Goh/3e8ccf9d3d843c9855c5d76ab66d3e775384da72"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/3e8ccf9d3d843c9855c5d76ab66d3e775384da72?fields=citationCount" alt="citation"></a></td>
</tr>
</tbody></table>
<h3 id="新领域应用"><a href="#新领域应用" class="headerlink" title="新领域应用"></a>新领域应用</h3><table>
<thead>
<tr>
<th>已读</th>
<th>年份</th>
<th>名字</th>
<th>简介</th>
<th>引用</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td>2016</td>
<td><a target="_blank" rel="noopener" href="https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf">AlphaGo</a></td>
<td>强化学习出圈</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Mastering-the-game-of-Go-with-deep-neural-networks-Silver-Huang/846aedd869a00c09b40f1f1f35673cb22bc87490"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/846aedd869a00c09b40f1f1f35673cb22bc87490?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2020</td>
<td><a target="_blank" rel="noopener" href="https://discovery.ucl.ac.uk/id/eprint/10089234/1/343019_3_art_0_py4t4l_convrt.pdf">AlphaFold</a></td>
<td>赢得比赛的的蛋白质3D结构预测</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Improved-protein-structure-prediction-using-from-Senior-Evans/3a083d843f891b3574494c385699c21766ce8b7a"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/3a083d843f891b3574494c385699c21766ce8b7a?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2021</td>
<td><a target="_blank" rel="noopener" href="https://www.nature.com/articles/s41586-021-03819-2.pdf">AlphaFold 2</a></td>
<td>原子级别精度的蛋白质3D结构预测</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Highly-accurate-protein-structure-prediction-with-Jumper-Evans/dc32a984b651256a8ec282be52310e6bd33d9815"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/dc32a984b651256a8ec282be52310e6bd33d9815?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2021</td>
<td><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2107.03374.pdf">Codex</a></td>
<td>使用注释生成代码</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Evaluating-Large-Language-Models-Trained-on-Code-Chen-Tworek/acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2021</td>
<td><a target="_blank" rel="noopener" href="https://www.nature.com/articles/s41586-021-04086-x.pdf">指导数学直觉</a></td>
<td>分析不同数学物体之前的联系来帮助发现新定理</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Advancing-mathematics-by-guiding-human-intuition-AI-Davies-Velickovic/f672b8fb430606fee0bb368f16603531ce1e90c4"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/f672b8fb430606fee0bb368f16603531ce1e90c4?fields=citationCount" alt="citation"></a></td>
</tr>
<tr>
<td></td>
<td>2022</td>
<td><a target="_blank" rel="noopener" href="https://storage.googleapis.com/deepmind-media/AlphaCode/competition_level_code_generation_with_alphacode.pdf">AlphaCode</a></td>
<td>媲美一般程序员的编程解题水平</td>
<td><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Competition-Level-Code-Generation-with-AlphaCode-Li-Choi/5cbe278b65a81602a864184bbca37de91448a5f5"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https://api.semanticscholar.org/graph/v1/paper/5cbe278b65a81602a864184bbca37de91448a5f5?fields=citationCount" alt="citation"></a></td>
</tr>
</tbody></table>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://redocting.com">ReDocTing</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://redocting.com/2024/06/13/paper/">https://redocting.com/2024/06/13/paper/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2024/06/04/hello-world2/" title="论文"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next</div><div class="next_info">论文</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">ReDocTing</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">3</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%98%85%E8%AF%BB%E5%88%97%E8%A1%A8"><span class="toc-number">1.</span> <span class="toc-text">阅读列表</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-CNN"><span class="toc-number">1.0.1.</span> <span class="toc-text">计算机视觉 - CNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-Transformer"><span class="toc-number">1.0.2.</span> <span class="toc-text">计算机视觉 - Transformer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.0.3.</span> <span class="toc-text">生成模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-Object-Detection"><span class="toc-number">1.0.4.</span> <span class="toc-text">计算机视觉 - Object Detection</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.0.5.</span> <span class="toc-text">计算机视觉 - 对比学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3"><span class="toc-number">1.0.6.</span> <span class="toc-text">计算机视觉 - 视频理解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E6%A8%A1%E6%80%81%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.0.7.</span> <span class="toc-text">多模态学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-Transformer"><span class="toc-number">1.0.8.</span> <span class="toc-text">自然语言处理 - Transformer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%B3%BB%E7%BB%9F"><span class="toc-number">1.0.9.</span> <span class="toc-text">系统</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.0.10.</span> <span class="toc-text">图神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-number">1.0.11.</span> <span class="toc-text">优化算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B0%E9%A2%86%E5%9F%9F%E5%BA%94%E7%94%A8"><span class="toc-number">1.0.12.</span> <span class="toc-text">新领域应用</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/13/paper/" title="阅读列表">阅读列表</a><time datetime="2024-06-13T10:10:22.715Z" title="Created 2024-06-13 19:10:22">2024-06-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/04/hello-world2/" title="论文">论文</a><time datetime="2024-06-04T04:51:14.158Z" title="Created 2024-06-04 13:51:14">2024-06-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/05/26/hello-world/" title="Hello World">Hello World</a><time datetime="2024-05-26T05:08:35.430Z" title="Created 2024-05-26 14:08:35">2024-05-26</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By ReDocTing</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>